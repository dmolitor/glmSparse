---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

# Load data for example regressions
library(dplyr)
library(readr)
pums_dat <- read_delim("~/../Downloads/USCensus1990.data.txt", 
                       delim = ",", 
                       n_max = 50000) %>%
  select(-caseid) %>%
  mutate(across(.cols = -iSex, .fns = as.factor))
```

# glmSparse

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of glmSparse is to provide common methods such as `print`, `summary`,
`broom::tidy`, for sparse/dense GLM models implemented in the
[MatrixModels](https://cran.r-project.org/web/packages/MatrixModels/index.html) 
package. For more information on this particular implementation of GLM models,
see the documentation for `MatrixModels::glm4()`.

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("dmolitor/glmSparse")
```
## Sparse vs. Dense

The primary feature of `glmSparse()` is that it allows the user the create a sparse model
matrix and fit a generalized linear model using this sparse matrix. In cases where
the model matrix is highly sparse this will be much more memory/computationally
efficient. For a more in-depth discussion of sparse matrices and their benefits, see
[this write-up](https://cran.r-project.org/web/packages/Matrix/vignettes/sparseModels.pdf).
Using a quick example we can confirm that the results of `glm()` and `glmSparse()`
are the same.

```{r Comparison}
library(broom)
library(bench)
library(glmSparse)
library(Matrix)
library(speedglm)

x <- sparse.model.matrix(mpg ~ ., data = mtcars)
y <- mtcars$mpg

# stats::glm implementation
glmBase <- glm(mpg ~ ., 
               family = gaussian, 
               data = mtcars)
# glmSparse formula interface
glmSparse_form <- glmSparse(mpg ~ .,
                            family = gaussian,
                            data = mtcars)
# glmSparse alternate interface
glmSparse_alt <- glmSparse(x = x,
                           y = y,
                           family = gaussian)

# Compare the coefficient tables
tidy(glmBase)
tidy(glmSparse_form)
tidy(glmSparse_alt)
```
The results are identical!

## Alternatives ##

The following example compares `glmSparse` to `glm` and `speedglm` using a 
sub-sample of the [PUMS data for the 1990 census](https://www.census.gov/data/datasets/1990/dec/pums.html) to classify an individual's sex. 
This provides a tractable example of a case where there will be significant sparsity, as each variable
is coded as a factor variable, often with multiple levels. When fully expanded,
the model matrix will be fairly sparse and `glmSparse` can take advantage of
this. The following gives us a quick overview of the modeling data
```{r}
str(pums_dat[, c(1, 4:7, 56)])
```

Using the `bench` package, we can benchmark the fitting time and memory
usage of `glmSparse` compared to its `glm` and `speedglm` counterparts.

```{r Compare glm with glmSparse and speedglm, echo=TRUE, message=FALSE, warning=FALSE}
benchmarks <- mark(
  glm(iSex ~ .,
        data = pums_dat[, c(1, 4:7, 56)],
        family = binomial),
  glmSparse(iSex ~ .,
              data = pums_dat[, c(1, 4:7, 56)],
              family = binomial),
  speedglm(iSex ~ .,
             data = pums_dat[, c(1, 4:7, 56)],
             family = binomial()),
  iterations = 100,
  check = FALSE
)

# Arrange implementations from fastest (median) to slowest
benchmarks %>%
  mutate(implementation = c("glm", "glmSparse", "speedglm")) %>%
  `[`(, c(14, 2:3, 5, 7:9)) %>%
  arrange(median) %>%
  print()
```
